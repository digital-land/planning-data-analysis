{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Plan Data Collection Process\n",
    "\n",
    "## Step 1: Manual Data Collection\n",
    "\n",
    "We begin by visiting the following website:  \n",
    "[UK Local Authorities](https://www.planning.data.gov.uk/organisation/#local-authority)\n",
    "\n",
    "### Process:\n",
    "1. Click on each local authority listed on the page.\n",
    "2. Note the **three-letter reference** for each local authority.\n",
    "3. Follow the link to the local authority’s website and search for their **local plans**.\n",
    "4. Identify the page that contains information and documents relating to the local plan.\n",
    "5. Record the following details in a spreadsheet:\n",
    "   - **Start date**, **End date**, and **Adoption date** of the local plan.\n",
    "   - The **URL** of the page containing this information.\n",
    "   - The three-letter reference.\n",
    "\n",
    "This manual process builds the initial dataset, which we will populate further using automated scripts.\n",
    "\n",
    "## Step 2: Automating Data Population\n",
    "\n",
    "After manually collecting the local plan data, we use a Python script to automate the process of populating additional information in the spreadsheet.\n",
    "\n",
    "### What the script does:\n",
    "1. **Load Data:** It loads two CSV files — one with the manually collected data and another with details about UK local authorities.\n",
    "2. **Select Relevant Columns:** The script narrows down the second CSV to only include columns we need, such as the local authority code and official name.\n",
    "3. **Prepare Data for Matching:** It extracts the three-letter codes from both datasets and ensures they are in the correct format for merging.\n",
    "4. **Merge Data:** A left merge is performed using the three-letter codes to match the official names of the local authorities with the corresponding information in the manually collected data.\n",
    "5. **Clean and Populate Columns:** The script fills in details like organisation names and generates unique references for each local plan based on the organisation and the start date of the local plan.\n",
    "6. **Save Updated Data:** After processing, the updated dataset is saved as a new CSV file for further use.\n",
    "\n",
    "## Step 3: Extracting Document Links from Local Plan Pages\n",
    "\n",
    "The next step involves extracting links to the local plan documents from the web pages we collected. Another Python script is used to:\n",
    "\n",
    "1. **Fetch the Webpage:** The script downloads the content of the webpage corresponding to each local authority.\n",
    "2. **Extract Document Links:** It looks for document links such as PDFs, Word files, and other relevant documents.\n",
    "3. **Clean the Text:** The link text is cleaned and standardised to remove irrelevant characters (e.g., file sizes, extra spaces).\n",
    "4. **Generate References:** Each document is assigned a reference number, combining the local authority reference and a counter.\n",
    "5. **Fuzzy Matching:** The script attempts to match the document titles with official local plan names from a reference dataset, ensuring proper categorisation of the documents.\n",
    "6. **Store the Results:** The extracted data, including document links and matched references, is saved into a new CSV file.\n",
    "\n",
    "### Error Handling:\n",
    "During the extraction process, if the script encounters an error (e.g., the webpage cannot be accessed), the URL is added to a list of failed URLs. This list is saved in a separate CSV file for further manual review.\n",
    "\n",
    "## Step 4: Completing the Data\n",
    "\n",
    "Once the automated process is complete, the final dataset contains a full list of document URLs for local plans from each local authority. Any missing or failed entries can be manually reviewed and updated to ensure completeness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Note: Some of these libraries will need to be pip installed before running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "from fuzzywuzzy import process\n",
    "import warnings\n",
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Plan Sheet Completion Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df0 = pd.read_csv(\"data/local_plan_manual_collection.csv\")\n",
    "df1 = pd.read_csv(\"documents/uk_local_authorities_future.csv\")\n",
    "\n",
    "# Only select relevant columns from df1\n",
    "df1 = df1[[\"local-authority-code\", \"official-name\"]]\n",
    "\n",
    "# Ensure 'organisation' and 'local-authority-code' are treated as strings\n",
    "df0['organisation'] = df0['organisation'].astype(str)\n",
    "df1['local-authority-code'] = df1['local-authority-code'].astype(str)\n",
    "\n",
    "# Extract the codes from both df0 and df1\n",
    "df0['org_code'] = df0['organisation'].str.extract(r'([A-Z]{3,4})')\n",
    "df1['org_code'] = df1['local-authority-code'].str.extract(r'([A-Z]{3,4})')\n",
    "\n",
    "# Perform a left merge on the extracted 3-letter codes\n",
    "df = pd.merge(df0, df1[['org_code', 'official-name']], on='org_code', how='left')\n",
    "\n",
    "# Copy the 'official-name' column into 'organisation-name'\n",
    "df['organisation-name'] = df['official-name']\n",
    "\n",
    "# Ensure 'period-start-date' is numeric and convert it to an integer without decimals\n",
    "df['period-start-date'] = pd.to_numeric(df['period-start-date'], errors='coerce')  # Convert to numeric, forcing invalid to NaN\n",
    "df['period-start-date'] = df['period-start-date'].fillna(0).astype(int)  # Replace NaN with 0 or another value, then cast to int\n",
    "\n",
    "# Populating the 'reference' column \n",
    "df['slug'] = df['organisation-name'].apply(lambda x: slugify(str(x)))\n",
    "df['reference'] = df['slug'] + \"-local-plan-\" + df['period-start-date'].astype(str)\n",
    "df.drop('slug', axis=1, inplace=True)\n",
    "\n",
    "# Populating the 'name' column \n",
    "df['name'] = df['official-name'] + \" Local Plan \" + df['period-start-date'].astype(str)\n",
    "\n",
    "# Prepend 'local-authority:' to each entry in the 'organisation' column\n",
    "df['organisation'] = \"local-authority:\" + df['organisation']\n",
    "\n",
    "# Drop the 'org_code' and 'official-name' columns\n",
    "df = df.drop(columns=['org_code', 'official-name'])\n",
    "\n",
    "# Filter rows where 'documentation-url' is empty or missing\n",
    "df_missing_url = df[df['documentation-url'].isna() | df['documentation-url'].eq('')]\n",
    "\n",
    "# Save the rows without 'documentation-url' to a separate CSV file\n",
    "df_missing_url.to_csv('data/missing_documentation_url.csv', index=False)\n",
    "\n",
    "# Drop rows where 'documentation-url' is empty or missing from the original dataframe\n",
    "df = df[~df['documentation-url'].isna() & ~df['documentation-url'].eq('')]\n",
    "\n",
    "# Save the updated dataframe to a CSV file\n",
    "df.to_csv('data/outputs/local_plan.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Plans Documents Sheet Data Scrape and Completion Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as data/outputs/local_plan_documents_finals.csv\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the provided text by replacing or removing unwanted characters.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', \"'\", text)  # Replace all non-ASCII characters with an apostrophe\n",
    "    text = re.sub(r'\\[\\s*pdf\\s*\\]', '', text, flags=re.IGNORECASE)  # Remove [pdf] tags\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalise any excessive spaces\n",
    "    text = re.sub(r'\\(\\d+(,\\d{3})?KB\\)|\\d+(,\\d{3})?KB|\\d+MB', '', text)  # Remove file sizes like (63KB), 63KB, or 12MB\n",
    "\n",
    "    # Split the text into words and remove apostrophes at the end of each word\n",
    "    words = text.split()\n",
    "    cleaned_words = [word.rstrip(\"'\") for word in words]  # Remove apostrophe at the end of each word\n",
    "\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def extract_links_from_page(url, plan_prefix, reference_data):\n",
    "    \"\"\"\n",
    "    Extracts all document links from a webpage, cleans the text associated with each link,\n",
    "    and matches the text with a reference from an external CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the webpage to scrape.\n",
    "    plan_prefix (str): The prefix to use for naming references.\n",
    "    reference_data (pd.DataFrame): The dataframe containing the reference data to match with.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of lists, where each sublist contains the reference, plan prefix, cleaned text,\n",
    "          full URL of the document, the input URL, and the matched reference from the CSV.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "}\n",
    "\n",
    "    response = requests.get(url, verify=False)\n",
    "    #response = requests.get(url, verify=certifi.where())\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all <a> tags that contain href attributes\n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    link_data = []\n",
    "    counter = 1\n",
    "    \n",
    "    # Process href links and check if they contain 'pdf', 'doc', 'document', or 'file'\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        full_url = urljoin(url, href)  # Handle relative URLs\n",
    "        \n",
    "        # Check if 'pdf', 'doc', 'document', or 'file' is in the URL\n",
    "        if any(x in href.lower() for x in ['pdf', 'doc', 'document', 'file']):\n",
    "            text = link.get_text(strip=True)  # Get the link text\n",
    "            text = clean_text(text)  # Clean the text\n",
    "            reference = f\"{plan_prefix}-{counter}\"  # Create the reference using the plan prefix and counter\n",
    "            \n",
    "            # Fuzzy match the text to the \"name\" column in reference_data\n",
    "            match = process.extractOne(text, reference_data['name'])\n",
    "            matched_reference = reference_data.loc[reference_data['name'] == match[0], 'reference'].values[0] if match else None\n",
    "\n",
    "            link_data.append([reference, plan_prefix, text, full_url, url, matched_reference])  # Add the matched reference\n",
    "            counter += 1  # Increment the counter for each link\n",
    "\n",
    "    return link_data\n",
    "\n",
    "# Main script logic\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your main DataFrame (df) containing 'reference' and 'documentation-url'\n",
    "    df = pd.read_csv('')  \n",
    "\n",
    "    # Load the reference data from the CSV file (assumed to be the same for all rows)\n",
    "    reference_data = pd.read_csv('documents/development-plan-document-type.csv')\n",
    "\n",
    "    # Create an empty list to hold all extracted link data across all iterations\n",
    "    all_link_data = []\n",
    "\n",
    "    # List to hold tuples of failed URLs and their associated plan_prefix\n",
    "    failed_urls = []\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        ref = row['reference']  # Reference from the current row\n",
    "        url = row['documentation-url']  # Documentation URL from the current row\n",
    "\n",
    "        try:\n",
    "            # Attempt to extract links from the current page\n",
    "            link_data = extract_links_from_page(url, ref, reference_data)\n",
    "            \n",
    "            # Append the extracted data to the all_link_data list\n",
    "            all_link_data.extend(link_data)\n",
    "        except Exception as e:\n",
    "            # If an error occurs, print the error and add the URL and plan_prefix to the failed list\n",
    "            print(f\"Error processing {url} with plan {ref}: {e}\")\n",
    "            failed_urls.append((ref, url))\n",
    "            continue  # Move on to the next URL\n",
    "\n",
    "    # Create a DataFrame from the combined list of link data\n",
    "    final_df = pd.DataFrame(all_link_data, columns=['reference', 'plan', 'text', 'url', 'input_url', 'matched_reference'])\n",
    "    \n",
    "    # Populate blank rows with 'supplementary-planning-documents'\n",
    "    final_df['matched_reference'].fillna('supplementary-planning-documents', inplace=True)\n",
    "    \n",
    "    # Rename columns to specification\n",
    "    final_df.rename(columns={'text':'name', \n",
    "                             'url':'document-url', \n",
    "                             'input_url':'documentation-url', \n",
    "                             'matched_reference':'document-types'}, \n",
    "                   inplace=True\n",
    "                   )\n",
    "\n",
    "    # Save the final DataFrame as a CSV file\n",
    "    output_path = ''\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Data saved as {output_path}\")\n",
    "\n",
    "    # Save failed URLs along with their plan_prefix to a CSV file\n",
    "    if failed_urls:\n",
    "        # Create a DataFrame for failed URLs with columns 'reference' and 'documentation-url'\n",
    "        failed_df = pd.DataFrame(failed_urls, columns=['reference', 'documentation-url'])\n",
    "        failed_urls_path = 'data/failed_urls.csv' \n",
    "        failed_df.to_csv(failed_urls_path, index=False)\n",
    "\n",
    "        print(f\"Failed URLs saved to {failed_urls_path}\")\n",
    "\n",
    "        # Print out the list of failed URLs\n",
    "        print(\"\\nThe following URLs failed during processing:\")\n",
    "        for ref, failed_url in failed_urls:\n",
    "            print(f\"Reference: {ref}, URL: {failed_url}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
